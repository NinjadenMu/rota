# Rota AI
Rota, or Roman Tic-Tac-Toe, is a game played on a circular board with 8 spots evenly distributed around the circumference of the circle and one spot in the middle.  Each player has 3 pieces, and they take turns placing down their pieces on unoccupied spots of their choice at the start of the game.  Once all 6 pieces are placed, players may slide their pieces to any unoccupied adjacent spot (there are 3 adjacent spots for all pieces on the circumference: the 2 spots surrounding the spot the piece is currently in, and the center.  There are 8 adjacent spots for the center: every spot on the circumference).  The objective of the game is to move all 3 pieces such that they're collinear (one piece will have to be in the center).  This seemingly simple rule set, unlike Tic-Tac-Toe, actually leads to fairly interesting and difficult gameplay.  For example, the side that occupies the center spot typically has an advantage, but if the other side can "trap" the center-holding player's other 2 pieces, that player will be forced to vacate the center.  

I've illustrated an example of a Rota position, and drawn out the posssible moves from one spot (although any red piece could be moved).  The numbers assigned to each spot are used to play with a text interface.  
<img src="https://github.com/NinjadenMu/rota/blob/main/example.png" width="512">

Here's a good video explaing Rota, since I'm sure I didn't do a great job: [https://www.youtube.com/watch?v=Z6jS9vu7P1M](https://www.youtube.com/watch?v=Z6jS9vu7P1M)

I'm fairly certain Rota is a tie if played perfectly - the players should be able to make moves forever while preventing the other side from winning.  Although this rule isn't in the original ruleset, I added a rule to keep games from lasting forever: if a position is repeated twice, the game ends in a draw (another word for tie).  This rule is inspired by chess' threefold-repetition rule.

To play this game well, an AI has to be able to look several moves ahead (I'll call the AI an engine, continuing with chess terminology).  I accomplish this with a fairly inefficient implementation of a minimax tree search with alpha-beta pruning.  To detect repetitions, I also implement a transposition table.  Transposition tables were originally designed to make chess engines more efficient by cutting out redundant computations by storing evaluations of a position the first time it's calculated - of course, I take advantage of this as well.  

Originally, I had no evaluation function: the minimax tree search would terminate if either side won.  However, that winning position could be dozens of moves in the future, and because the game tree grows exponentially, becomes infeasible to search to.  To solve this, the search cuts off at a certain depth: if no side has won, the position is assumed to be a draw.  This assumption has serious issues - the engine might assume a position that can be won after some additional moves beyond the search depth is a draw, and go down a losing line.  Additionally, even if two moves lead to a draw, one could be preferable.  For example, if move a forces the opponent to find an only move to draw (let's say the opponent has 6 legal moves, of which 1 leads to a draw, 5 lead to a loss), and move b allows the opponent to make a draw by playing any of their legal moves, move a is obviously preferable, since there's a greater probability that the opponent makes a mistake and loses.  However, the naive approach which can only return won, lost and draw,  doesn't take this into account. 

To resolve these issues, I train a neural network to evaluate how close to winning/losing a position is.  Then, when the search tree reaches it's max depth, it uses the neural network to evaluate the position instead of assuming the position to be a draw.  For training data, I use a Monte-Carlo Markov-Chain to sample thousands of random positions at different stages of the game.  I then use my naive tree search to assess whether the game is won, drawn, or lost.  Even though the naive tree search is not perfect, it still is searching several moves ahead, and is thus much better than assuming the position to be a draw.  The neural network is then trained to predict a value between -1 and 1, with -1 representing a loss and 1 representing a win.  Because the network won't be able to achieve perfect accuracy, it might evaluate some positions somewhere in this interval (e.g. 0.25) - this is actually desired behavior, since it incorporates some of that probability into the engine's understanding of a position.  This neural network works quite well - searching to depth 2 with the network beats searching to depth 4 without it.  From a human perspective, the neural network also seems to lead to more challenging play.  Without the network, the engine frequently abandons the center spot in drawn positions if abandoning the center also leads to a draw.  However, the engine with the neural network doggedly holds onto the center even in drawn positions, since it knows that it'll have a slightly better chance of winning that way.

The game rules and search are implemented in game.py (this should probably be modularized).  I've included a pkl file for the generated data, and the trained neural network weights.  **Please play the game by running game.py!**

I also actually implemented code to get the principal variation (PV) in search - this is unnecessary for the engine in it's current state, but it means I can extend the engine to use iterative deepening in it's search in the future. 
